{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l1\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# create session explicitly and keep a reference\n",
    "# so we can access and evaluate tensors directly \n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables \n",
    "model_name = \"streeteye_lstm\"\n",
    "#data_file = \"lstm_dump_test.txt\"\n",
    "data_file = \"dump_2017_words.txt\"\n",
    "\n",
    "checkpoint_dir = \"/home/ubuntu/mount/Notebooks/checkpoints\"\n",
    "tensorboard_dir =\"/home/ubuntu/mount/Notebooks/tensorboard\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded data.\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# 1. load data\n",
    "############################################################\n",
    "\n",
    "# load dataset\n",
    "print(\"Loading data...\")\n",
    "data=[]\n",
    "y=[]\n",
    "\n",
    "# count words\n",
    "c = collections.Counter()\n",
    "\n",
    "with open(data_file, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        l = line.rstrip('\\n').split(\",\")\n",
    "        label = l.pop(0)\n",
    "        # skip empty headlines\n",
    "        if len(l[0]) == 0:\n",
    "            continue\n",
    "        if '' in l:\n",
    "            l = [w for w in l if w]\n",
    "        data.append(l)\n",
    "        y.append(label)\n",
    "        c.update(l)\n",
    "        \n",
    "print(\"Loaded data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', -1], ('domain_otherdomain', 119708), ('subsource_othersubsource', 47862), ('trump', 21141), ('with', 10761), ('domain_youtube.com', 8908), ('us', 8434), ('2017', 7862), ('from', 7768), ('subsource_memeorandum', 7712)]\n",
      "[('hazard', 17), ('alexei', 17), ('molly', 17), ('expel', 17), ('champ', 17), ('admiral', 17), ('conversational', 17), ('memorable', 17), ('wharton', 17), ('torn', 17)]\n"
     ]
    }
   ],
   "source": [
    "# create a list of top words        \n",
    "vocabulary_size = 10000 # set this to have ~20 for least popular\n",
    "count = [['UNK', -1]]\n",
    "count.extend(c.most_common(vocabulary_size - 1))\n",
    "print(count[:10])\n",
    "print(count[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "# map words into a dict of ints\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "\n",
    "data_embeddings=[]\n",
    "unk_count = 0\n",
    "\n",
    "for obs in data:\n",
    "    embedlist = []\n",
    "    for word in obs:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        embedlist.append(index)\n",
    "    data_embeddings.append(embedlist)\n",
    "        \n",
    "count[0][1] = unk_count\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "trump\n"
     ]
    }
   ],
   "source": [
    "print(dictionary['trump'])\n",
    "print(reverse_dictionary[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7ff3ed1b0c90>]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqZuhIUnq9n9K8+wLvazDjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3ed1b0450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ls = (map(len, data_embeddings))\n",
    "pd.DataFrame(ls).hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218419, 120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 120\n",
    "X = sequence.pad_sequences(data_embeddings, maxlen=MAX_LENGTH)\n",
    "X[0]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218419, 1)\n",
      "Observations: 218419\n",
      "Features: 120\n",
      "Split into training, temp\n",
      "Split into xval, test\n"
     ]
    }
   ],
   "source": [
    "y=np.array(np.float32(y))\n",
    "\n",
    "y=y.reshape((y.shape[0],1))\n",
    "print(y.shape)\n",
    "num_labels=1\n",
    "\n",
    "num_obs, num_features = X.shape\n",
    "print(\"Observations: %d\\nFeatures: %d\" % (num_obs, num_features))\n",
    "\n",
    "# split into training, xval, test, 60/20/20\n",
    "print(\"Split into training, temp\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)\n",
    "print(\"Split into xval, test\")\n",
    "X_xval, X_test, y_xval, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "(131051, 120)\n",
      "Xval set\n",
      "(43684, 120)\n",
      "Test set\n",
      "(43684, 120)\n",
      "\n",
      "Training observations:  131051  \n",
      "Xval observations:  43684  \n",
      "Test observations:  43684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training set\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Xval set\")\n",
    "print(X_xval.shape)\n",
    "\n",
    "print(\"Test set\")\n",
    "print(X_test.shape)\n",
    "\n",
    "num_training_samples = X_train.shape[0]\n",
    "num_xval_samples = X_xval.shape[0]\n",
    "num_test_samples = X_test.shape[0]\n",
    "\n",
    "print (\"\\nTraining observations:  %d  \\nXval observations:  %d  \\nTest observations:  %d\\n\" % (num_training_samples, num_xval_samples, num_test_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize embeddings to pre-trained vals\n",
    "pkl_file = open('embeddings.pkl', 'rb')\n",
    "embeddings_dict, embeddings_reverse_dict, embeddings_data = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized 10000 embeddings\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "\n",
    "# +1 seems like an off-by-one somewhere\n",
    "embedding_matrix = np.zeros((len(dictionary) + 1, EMBEDDING_DIM))\n",
    "\n",
    "count = 0\n",
    "for word, i in dictionary.items():\n",
    "    #print(word)\n",
    "    embed_i = embeddings_dict.get(word)\n",
    "    if embed_i is not None:\n",
    "        embedding_vector = embeddings_data[i]\n",
    "        count +=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(\"initialized %d embeddings\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate model\n",
    "\n",
    "def create_model(lstm_size=30, lstm_reg_penalty=0.0, sigmoid_dropout=(1.0/3.0), sigmoid_reg_penalty=0.0001):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(len(dictionary) + 1, \n",
    "                        embedding_vector_length, \n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=MAX_LENGTH,\n",
    "                        trainable=True))\n",
    "    \n",
    "    # LSTM with lstm_size units\n",
    "    model.add(LSTM(lstm_size,\n",
    "                   kernel_regularizer=l1(lstm_reg_penalty)))\n",
    "    model.add(Dropout(sigmoid_dropout))\n",
    "    \n",
    "    model.add(Dense(1, \n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='TruncatedNormal', \n",
    "                    kernel_regularizer=l1(sigmoid_reg_penalty)))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectThreshold (logits, labels, beta=(2.0/3)):\n",
    "    # return threshold, f-score that yields best F-score\n",
    "    # predict using true if >= threshold\n",
    "\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(labels, logits)\n",
    "    bb = beta**2\n",
    "    f1_scores = (1 + bb) * precision * recall / (bb * precision + recall)\n",
    "    f1_scores = np.nan_to_num(f1_scores)\n",
    "    \n",
    "    best_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_index]\n",
    "    best_score = f1_scores[best_index]\n",
    "    return (best_threshold, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 120, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 16)                20288     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,020,605.0\n",
      "Trainable params: 3,020,605.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "12:20:18 Starting (unfrozen)...\n",
      "LSTM units 16\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 168s - loss: 0.3287 - acc: 0.9649 - val_loss: 0.1515 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.1406 - acc: 0.9726 - val_loss: 0.1334 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.1284 - acc: 0.9726 - val_loss: 0.1220 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.0925 - acc: 0.9747 - val_loss: 0.0636 - val_acc: 0.9832\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.0545 - acc: 0.9855 - val_loss: 0.0533 - val_acc: 0.9837\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.0413 - acc: 0.9899 - val_loss: 0.0535 - val_acc: 0.9830\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.0325 - acc: 0.9926 - val_loss: 0.0590 - val_acc: 0.9831\n",
      "12:39:39 Best Xval loss epoch 4, value 0.053344\n",
      "12:39:39 Finished (unfrozen)...\n",
      "LSTM units 16\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "12:41:01 Train Accuracy 0.995, Train F1 0.907, f_score 0.923 (beta 0.667)\n",
      "[[127296    478]\n",
      " [   164   3113]]\n",
      "12:41:28 Xval Accuracy 0.983, Xval F1 0.667, f_score 0.711 (beta 0.667)\n",
      "[[42224   543]\n",
      " [  187   730]]\n",
      "Raw score 2 0.012430\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 120, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 16)                20288     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,020,605.0\n",
      "Trainable params: 20,305.0\n",
      "Non-trainable params: 3,000,300.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "12:41:28 Continuing (frozen)...\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 135s - loss: 0.0232 - acc: 0.9952 - val_loss: 0.0693 - val_acc: 0.9827\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 132s - loss: 0.0212 - acc: 0.9954 - val_loss: 0.0696 - val_acc: 0.9821\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0203 - acc: 0.9955 - val_loss: 0.0735 - val_acc: 0.9825\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0199 - acc: 0.9957 - val_loss: 0.0723 - val_acc: 0.9826\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0199 - acc: 0.9959 - val_loss: 0.0751 - val_acc: 0.9829\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0195 - acc: 0.9959 - val_loss: 0.0703 - val_acc: 0.9815\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0191 - acc: 0.9959 - val_loss: 0.0745 - val_acc: 0.9827\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0185 - acc: 0.9961 - val_loss: 0.0744 - val_acc: 0.9827\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0189 - acc: 0.9961 - val_loss: 0.0748 - val_acc: 0.9827\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 133s - loss: 0.0183 - acc: 0.9963 - val_loss: 0.0765 - val_acc: 0.9828\n",
      "13:03:43 Best Xval loss epoch 0, value 0.069291\n",
      "13:03:43 Finished (frozen)...\n",
      "LSTM units 16\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "13:05:05 Train Accuracy 0.997, Train F1 0.935, f_score 0.947 (beta 0.667)\n",
      "[[127352    345]\n",
      " [   108   3246]]\n",
      "13:05:32 Xval Accuracy 0.983, Xval F1 0.688, f_score 0.706 (beta 0.667)\n",
      "[[42118   452]\n",
      " [  293   821]]\n",
      "Raw score 2 0.012087\n",
      "13:05:32 Saving...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 120, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,042,957.0\n",
      "Trainable params: 3,042,957.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "13:05:39 Starting (unfrozen)...\n",
      "LSTM units 32\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 276s - loss: 0.2522 - acc: 0.9697 - val_loss: 0.1322 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      " 63488/131051 [=============>................] - ETA: 126s - loss: 0.1264 - acc: 0.9730"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-297bef845fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0;31m# save loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "embedding_vector_length = EMBEDDING_DIM\n",
    "\n",
    "for sig_reg_penalty in [0.00003]:\n",
    "    for dropout in [0.333]:\n",
    "        for lstm_units in [16, 32]:\n",
    "            for lstm_reg_penalty in [0.00000,]:\n",
    "                #0.000001, 0.000003, 0.00001, 0.00003]:\n",
    "                models = []\n",
    "                xval_losses = []\n",
    "\n",
    "                model = create_model(lstm_size=lstm_units, \n",
    "                                     lstm_reg_penalty=lstm_reg_penalty, \n",
    "                                     sigmoid_dropout=dropout, \n",
    "                                     sigmoid_reg_penalty=sig_reg_penalty)\n",
    "                print('%s Starting (unfrozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "                print (\"LSTM units %d\" % lstm_units)\n",
    "                print (\"LSTM reg_penalty %.8f\" % lstm_reg_penalty)\n",
    "                print (\"Sigmoid dropout %.4f\" %  dropout)\n",
    "                print (\"Sigmoid reg_penalty %.8f\" % sig_reg_penalty)\n",
    "\n",
    "                ##################################################################\n",
    "                # train end-to-end including embeddings until xval loss bottoms out\n",
    "                ##################################################################\n",
    "                \n",
    "                epochs = 10\n",
    "                for _ in range(epochs):\n",
    "                    fit = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=1, batch_size=1024)\n",
    "                    # save loss\n",
    "                    train_loss = fit.history['loss'][-1]\n",
    "                    train_acc = fit.history['acc'][-1]\n",
    "                    xval_loss = fit.history['val_loss'][-1]\n",
    "                    xval_acc = fit.history['val_acc'][-1]\n",
    "                    xval_losses.append(xval_loss)\n",
    "                    models.append(copy.copy(model))\n",
    "\n",
    "                    bestloss_index = np.argmin(xval_losses)\n",
    "                    bestloss_value = xval_losses[bestloss_index]\n",
    "\n",
    "                    # break if loss rises by 10% from best\n",
    "                    if xval_loss / bestloss_value > 1.1:\n",
    "                        break\n",
    "                    \n",
    "                # keep model from epoch with best xval loss\n",
    "                print (\"%s Best Xval loss epoch %d, value %f\" % (time.strftime(\"%H:%M:%S\"), bestloss_index, bestloss_value))\n",
    "                # go back one from best... best is usually already overfitted with .9 train f1\n",
    "                if bestloss_index > 0:\n",
    "                    bestloss_index -= 1\n",
    "                    print (\"%s Using epoch %d, value %f\" % (time.strftime(\"%H:%M:%S\"), bestloss_index, bestloss_value))\n",
    "                \n",
    "                model = models[bestloss_index]\n",
    "\n",
    "                print('%s Finished (unfrozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "                print (\"LSTM units %d\" % lstm_units)\n",
    "                print (\"LSTM reg_penalty %.8f\" % lstm_reg_penalty)\n",
    "                print (\"Sigmoid dropout %.4f\" %  dropout)\n",
    "                print (\"Sigmoid reg_penalty %.8f\" % sig_reg_penalty)                \n",
    "                \n",
    "                y_train_prob = model.predict(X_train)\n",
    "                \n",
    "                beta=(2.0/3.0) # penalize false positives more than false negatives\n",
    "                thresh, score = selectThreshold(y_train_prob, y_train, beta=beta)\n",
    "                y_train_pred = y_train_prob >= thresh\n",
    "\n",
    "                \n",
    "                print(\"%s Train Accuracy %.3f, Train F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_train_pred, y_train), \n",
    "                       sklearn.metrics.f1_score(y_train_pred, y_train),\n",
    "                       score, beta))\n",
    "                \n",
    "                print(sklearn.metrics.confusion_matrix(y_train_pred, y_train))\n",
    "\n",
    "                y_xval_prob = model.predict(X_xval)\n",
    "                \n",
    "                thresh, score = selectThreshold(y_xval_prob, y_xval, beta=beta)\n",
    "                y_xval_pred = y_xval_prob >= thresh\n",
    "                \n",
    "                print(\"%s Xval Accuracy %.3f, Xval F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "                       sklearn.metrics.f1_score(y_xval_pred, y_xval),\n",
    "                       score, beta))\n",
    "                \n",
    "                confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "                print(confusion_matrix)\n",
    "                false_positive = confusion_matrix[1][0]\n",
    "                false_negative = confusion_matrix[0][1]\n",
    "                true_positive = confusion_matrix[1][1]\n",
    "                raw_score = 1.0 * (true_positive - false_positive) / np.sum(confusion_matrix)\n",
    "                print (\"Raw score 2 %f\" % raw_score)\n",
    "\n",
    "                ##################################################################\n",
    "                # freeze embeddings, train LSTM until xval loss bottoms out\n",
    "                ##################################################################\n",
    "                elayer = model.layers[0]\n",
    "                \n",
    "                elayer.trainable = False\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                print(model.summary())\n",
    "\n",
    "                # Could also keep existing list of models in case further training makes it worse                \n",
    "                models = []\n",
    "                xval_losses = []\n",
    "                epochs = 20\n",
    "\n",
    "                print('%s Continuing (frozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "\n",
    "                for _ in range(epochs):\n",
    "                    fit = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=1, batch_size=1024)\n",
    "                    # save losses\n",
    "                    train_loss = fit.history['loss'][-1]\n",
    "                    train_acc = fit.history['acc'][-1]\n",
    "                    xval_loss = fit.history['val_loss'][-1]\n",
    "                    xval_acc = fit.history['val_acc'][-1]\n",
    "                    xval_losses.append(xval_loss)\n",
    "                    models.append(copy.copy(model))\n",
    "\n",
    "                    bestloss_index = np.argmin(xval_losses)\n",
    "                    bestloss_value = xval_losses[bestloss_index]\n",
    "    \n",
    "                    # break if loss rises by 10% from best\n",
    "                    if xval_loss / bestloss_value > 1.1:\n",
    "                        break\n",
    "                    \n",
    "                # keep model from epoch with best xval loss\n",
    "                print (\"%s Best Xval loss epoch %d, value %f\" % (time.strftime(\"%H:%M:%S\"), bestloss_index, bestloss_value))\n",
    "                model = models[bestloss_index]\n",
    "\n",
    "                print('%s Finished (frozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "                print (\"LSTM units %d\" % lstm_units)\n",
    "                print (\"LSTM reg_penalty %.8f\" % lstm_reg_penalty)\n",
    "                print (\"Sigmoid dropout %.4f\" %  dropout)\n",
    "                print (\"Sigmoid reg_penalty %.8f\" % sig_reg_penalty)                \n",
    "               \n",
    "                y_train_prob = model.predict(X_train)\n",
    "                \n",
    "                beta=(2.0/3.0) # penalize false positives more than false negatives\n",
    "                thresh, score = selectThreshold(y_train_prob, y_train, beta=beta)\n",
    "                y_train_pred = y_train_prob >= thresh\n",
    "                \n",
    "                print(\"%s Train Accuracy %.3f, Train F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_train_pred, y_train),\n",
    "                       sklearn.metrics.f1_score(y_train_pred, y_train),\n",
    "                       score, beta))\n",
    "                \n",
    "                print(sklearn.metrics.confusion_matrix(y_train_pred, y_train))\n",
    "\n",
    "                y_xval_prob = model.predict(X_xval)\n",
    "                \n",
    "                thresh, score = selectThreshold(y_xval_prob, y_xval, beta=beta)\n",
    "                y_xval_pred = y_xval_prob >= thresh\n",
    "\n",
    "                print(\"%s Xval Accuracy %.3f, Xval F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "                       sklearn.metrics.f1_score(y_xval_pred, y_xval),\n",
    "                       score, beta))\n",
    "                \n",
    "                confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "                print(confusion_matrix)\n",
    "                false_positive = confusion_matrix[1][0]\n",
    "                false_negative = confusion_matrix[0][1]\n",
    "                true_positive = confusion_matrix[1][1]\n",
    "\n",
    "                raw_score = 1.0 * (true_positive - false_positive) / np.sum(confusion_matrix)\n",
    "                print (\"Raw score 2 %f\" % raw_score)\n",
    "                \n",
    "                # save model to disk\n",
    "                print('%s Saving...' % time.strftime(\"%H:%M:%S\"))               \n",
    "                modelname = \"model_%d_%.6f_%.3f_%.6f\" % (lstm_units, lstm_reg_penalty, dropout, sig_reg_penalty)\n",
    "                model.save(\"%s.h5\" % modelname)\n",
    "                model.save_weights(\"%s_weights.h5\" % modelname)\n",
    "                with open(\"%s.json\" % modelname, \"wb\") as fjson:\n",
    "                    fjson.write(model.to_json()) \n",
    "                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 120, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 16)                20288     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,020,605.0\n",
      "Trainable params: 3,020,605.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "13:12:32 Starting (unfrozen)...\n",
      "LSTM units 16\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 165s - loss: 0.3354 - acc: 0.9654 - val_loss: 0.1526 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.1301 - acc: 0.9726 - val_loss: 0.1017 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.0804 - acc: 0.9770 - val_loss: 0.0661 - val_acc: 0.9805\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.0559 - acc: 0.9864 - val_loss: 0.0584 - val_acc: 0.9839\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.0432 - acc: 0.9905 - val_loss: 0.0566 - val_acc: 0.9832\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.0339 - acc: 0.9931 - val_loss: 0.0598 - val_acc: 0.9829\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 162s - loss: 0.0279 - acc: 0.9947 - val_loss: 0.0629 - val_acc: 0.9826\n",
      "13:31:38 Best Xval loss epoch 4, value 0.056614\n",
      "13:31:38 Finished (unfrozen)...\n",
      "LSTM units 16\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "13:33:00 Train Accuracy 0.996, Train F1 0.922, f_score 0.937 (beta 0.667)\n",
      "[[127341    421]\n",
      " [   119   3170]]\n",
      "13:33:27 Xval Accuracy 0.983, Xval F1 0.695, f_score 0.703 (beta 0.667)\n",
      "[[42072   415]\n",
      " [  339   858]]\n",
      "Raw score 2 0.011881\n",
      "13:33:27 Saving...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 120, 300)          3000300   \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 3,042,957.0\n",
      "Trainable params: 3,042,957.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "13:33:35 Starting (unfrozen)...\n",
      "LSTM units 32\n",
      "LSTM reg_penalty 0.00000000\n",
      "Sigmoid dropout 0.3330\n",
      "Sigmoid reg_penalty 0.00003000\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 273s - loss: 0.2437 - acc: 0.9660 - val_loss: 0.1320 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 271s - loss: 0.1279 - acc: 0.9726 - val_loss: 0.1313 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 270s - loss: 0.1230 - acc: 0.9726 - val_loss: 0.1307 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 366s - loss: 0.1149 - acc: 0.9726 - val_loss: 0.1160 - val_acc: 0.9709\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "131051/131051 [==============================] - 484s - loss: 0.0611 - acc: 0.9813 - val_loss: 0.0517 - val_acc: 0.9836\n",
      "Train on 131051 samples, validate on 43684 samples\n",
      "Epoch 1/1\n",
      "  3072/131051 [..............................] - ETA: 433s - loss: 0.0416 - acc: 0.9883"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bc7527f52ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0;31m# save loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# grid search - no freeze\n",
    "embedding_vector_length = EMBEDDING_DIM\n",
    "\n",
    "for sig_reg_penalty in [0.00003]:\n",
    "    for dropout in [0.333]:\n",
    "        for lstm_units in [16, 32]:\n",
    "            for lstm_reg_penalty in [0.00000,]:\n",
    "                #0.000001, 0.000003, 0.00001, 0.00003]:\n",
    "                models = []\n",
    "                xval_losses = []\n",
    "\n",
    "                model = create_model(lstm_size=lstm_units, \n",
    "                                     lstm_reg_penalty=lstm_reg_penalty, \n",
    "                                     sigmoid_dropout=dropout, \n",
    "                                     sigmoid_reg_penalty=sig_reg_penalty)\n",
    "                print('%s Starting (unfrozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "                print (\"LSTM units %d\" % lstm_units)\n",
    "                print (\"LSTM reg_penalty %.8f\" % lstm_reg_penalty)\n",
    "                print (\"Sigmoid dropout %.4f\" %  dropout)\n",
    "                print (\"Sigmoid reg_penalty %.8f\" % sig_reg_penalty)\n",
    "\n",
    "                ##################################################################\n",
    "                # train end-to-end including embeddings until xval loss bottoms out\n",
    "                ##################################################################\n",
    "                \n",
    "                epochs = 10\n",
    "                for _ in range(epochs):\n",
    "                    fit = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=1, batch_size=1024)\n",
    "                    # save loss\n",
    "                    train_loss = fit.history['loss'][-1]\n",
    "                    train_acc = fit.history['acc'][-1]\n",
    "                    xval_loss = fit.history['val_loss'][-1]\n",
    "                    xval_acc = fit.history['val_acc'][-1]\n",
    "                    xval_losses.append(xval_loss)\n",
    "                    models.append(copy.copy(model))\n",
    "\n",
    "                    bestloss_index = np.argmin(xval_losses)\n",
    "                    bestloss_value = xval_losses[bestloss_index]\n",
    "\n",
    "                    # break if loss rises by 10% from best\n",
    "                    if xval_loss / bestloss_value > 1.1:\n",
    "                        break\n",
    "                    \n",
    "                # keep model from epoch with best xval loss\n",
    "                print (\"%s Best Xval loss epoch %d, value %f\" % (time.strftime(\"%H:%M:%S\"), bestloss_index, bestloss_value))\n",
    "                \n",
    "                model = models[bestloss_index]\n",
    "\n",
    "                print('%s Finished (unfrozen)...' % time.strftime(\"%H:%M:%S\"))\n",
    "                print (\"LSTM units %d\" % lstm_units)\n",
    "                print (\"LSTM reg_penalty %.8f\" % lstm_reg_penalty)\n",
    "                print (\"Sigmoid dropout %.4f\" %  dropout)\n",
    "                print (\"Sigmoid reg_penalty %.8f\" % sig_reg_penalty)                \n",
    "                \n",
    "                y_train_prob = model.predict(X_train)\n",
    "                \n",
    "                beta=(2.0/3.0) # penalize false positives more than false negatives\n",
    "                thresh, score = selectThreshold(y_train_prob, y_train, beta=beta)\n",
    "                y_train_pred = y_train_prob >= thresh\n",
    "\n",
    "                print(\"%s Train Accuracy %.3f, Train F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_train_pred, y_train), \n",
    "                       sklearn.metrics.f1_score(y_train_pred, y_train),\n",
    "                       score, beta))\n",
    "                \n",
    "                print(sklearn.metrics.confusion_matrix(y_train_pred, y_train))\n",
    "\n",
    "                y_xval_prob = model.predict(X_xval)\n",
    "                \n",
    "                thresh, score = selectThreshold(y_xval_prob, y_xval, beta=beta)\n",
    "                y_xval_pred = y_xval_prob >= thresh\n",
    "                \n",
    "                print(\"%s Xval Accuracy %.3f, Xval F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (time.strftime(\"%H:%M:%S\"),\n",
    "                       sklearn.metrics.accuracy_score(y_xval_pred, y_xval), \n",
    "                       sklearn.metrics.f1_score(y_xval_pred, y_xval),\n",
    "                       score, beta))\n",
    "                \n",
    "                confusion_matrix = sklearn.metrics.confusion_matrix(y_xval_pred, y_xval)\n",
    "                print(confusion_matrix)\n",
    "                false_positive = confusion_matrix[1][0]\n",
    "                false_negative = confusion_matrix[0][1]\n",
    "                true_positive = confusion_matrix[1][1]\n",
    "                raw_score = 1.0 * (true_positive - false_positive) / np.sum(confusion_matrix)\n",
    "                print (\"Raw score 2 %f\" % raw_score)\n",
    "                \n",
    "                # save model to disk\n",
    "                print('%s Saving...' % time.strftime(\"%H:%M:%S\"))               \n",
    "                modelname = \"nofreeze_%d_%.6f_%.3f_%.6f\" % (lstm_units, lstm_reg_penalty, dropout, sig_reg_penalty)\n",
    "                model.save(\"%s.h5\" % modelname)\n",
    "                model.save_weights(\"%s_weights.h5\" % modelname)\n",
    "                with open(\"%s.json\" % modelname, \"wb\") as fjson:\n",
    "                    fjson.write(model.to_json()) \n",
    "                    \n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model and evaluate in test set\n",
    "y_test_prob = model.predict(X_test)\n",
    "beta=(2.0/3.0) # penalize false positives more than false negatives\n",
    "\n",
    "y_test_pred = y_test_prob >= thresh\n",
    "print(\"Test Accuracy %.3f, Test F1 %.3f, f_score %.3f (beta %.3f)\" % \n",
    "                      (sklearn.metrics.accuracy_score(y_test_pred, y_test), \n",
    "                       sklearn.metrics.f1_score(y_test_pred, y_test),\n",
    "                       score, beta))\n",
    "                \n",
    "print(sklearn.metrics.confusion_matrix(y_test_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
